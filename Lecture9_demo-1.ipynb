{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network lecture demo/Lab\n",
    "\n",
    "In this demo we will use MLP to predict the orientation of a face in a 32 by 32 pixel image. The data set used is from: [Chaper 4 of Machine Learning book by Tom Mitchell](http://www.cs.cmu.edu/~tom/faces.html)\n",
    "\n",
    "The faces of 20 different people are captured at 4 orientations: Left, Right, Up, Straight. Images from each individual is in a separate folder and the label (orientation) for a specific image is given by the image name.  `glickman_up_neutral_sunglasses.pgm -> up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.1.0'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Face dataset. The dataset can be downloaded from canvas. You can upload it to the notebook work environment and unzip using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('./faces_TM.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the validation-training data. A good practice would be to hold out some individuals for validation. This will eliminate possible overlaps between train/test splits. Lets hold out the last 5 individuals for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_persons = ['sz24', 'megak', 'night', 'choon', 'kawamura']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the sub folders and read all the image names and create a data frame with relevant information that can be used for training and testing the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "image_list = []\n",
    "for filepath in glob.glob('./faces_TM/*/*.jpg', recursive=True): #assuming gif\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    person = filepath.split(\"/\")[-2]\n",
    "    label = filename.split(\"_\")[1]\n",
    "    # print(filepath, person, label)\n",
    "    test_train = person in test_persons\n",
    "    image_list.append(filepath, label, int(test_train))\n",
    "    \n",
    "# Create a data frame\n",
    "data = pd.DataFrame(data=image_list, columns=['image_path', 'label', 'isTest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some random images and observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1152x288 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_inx = np.random.choice(100, 4)\n",
    "rand_data = data.loc[r_inx,'image_path']\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for i, image_path in enumerate(rand_data):\n",
    "    im = np.asarray(Image.open(image_path))\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(im,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string labels to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "d = {'left':0, 'right':1, 'straight':2, 'up':3}\n",
    "data['labels_num'] = data['label'].map(d, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate two data frames for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train size: 624, Test size: 0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_df = data[data['isTest']==0].reset_index()\n",
    "validation_df = data[data['isTest']==1].reset_index()\n",
    "print('Train size: {}, Test size: {}'.format(train_df.shape[0], validation_df.shape[0] ) )\n",
    "N_train_images = train_df.shape[0]\n",
    "N_val_images = validation_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now design and train an extremely simple neural network.\n",
    "Two of the choices are made for us by the data.\n",
    "We have 28x28 features and four classes, so the input layer must have 784 units, and the output layer must have 4 units.\n",
    "We only have to define the hidden layers.\n",
    "We're only going to have one hidden layer for this project, and we'll give it 64 nodes/neurons/units.\n",
    "\n",
    "Keras provides two methods to define a model (sequential and functional API) and we are going to use the functional API (sequential is much simpler and you should try by yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 3)]       0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 50,500\n",
      "Trainable params: 50,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Lambda\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "\n",
    "# Input layer\n",
    "input_ = Input(shape=(28, 28, 3)) # This is the input shape\n",
    "input_slice = Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1, name=None))(input_)\n",
    "x = Flatten()(input_slice)  # This will convert the 28x28 input to a vector of  dimension 784\n",
    "\n",
    "# Hidden layer\n",
    "h = Dense(64)(x)\n",
    "h = Activation('sigmoid')(h)\n",
    "\n",
    "# Output layer\n",
    "out_ = Dense(4)(h)\n",
    "out_ = Activation('softmax')(out_)\n",
    "\n",
    "# Define model\n",
    "model_orig = Model(inputs=input_, outputs=out_)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_orig.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=[categorical_accuracy])\n",
    "\n",
    "\n",
    "model_orig.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model with the sequential API (uncomment and try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "# from tensorflow.keras import regularizers, optimizers\n",
    "\n",
    "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# model_orig = Sequential()\n",
    "# model_orig.add(Flatten(input_shape=(28, 28, 3)))\n",
    "# model_orig.add(Dense(64))\n",
    "# model_orig.add(Activation('sigmoid'))\n",
    "# model_orig.add(Dense(4))\n",
    "# model_orig.add(Activation('softmax'))\n",
    "# model_orig.compile(loss='binary_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=[categorical_accuracy])\n",
    "#\n",
    "# model_orig.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many optimizers that are available but for now, we will use rmsprop.\n",
    "You will work with other optimizers in experimental section of this tutorial.\n",
    "Here, we have used categorical_crossentropy because there are more than two categories in the output variable.\n",
    "\n",
    "As are going to work with much larger and more complicated data set, that neural network are more suited towards,\n",
    " we need to write efficient code to load the data in batches to memory. This is done in keras using Image data generators. \n",
    "\n",
    "To help we will define a loading function that takes the data frames we defined earlier:\n",
    "\n",
    "It seems that dataloader does not like to read one channel images. It automatically converts them to 3-channel images. Lets ignore this for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Found 624 validated image filenames belonging to 20 classes.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6a8b74067ed3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         class_mode='categorical')\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m    681\u001b[0m             \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m             \u001b[0mvalidate_filenames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_filenames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m         )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_valid_filepaths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclass_mode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multi_output\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;31m# build an index of all the unique classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py\u001b[0m in \u001b[0;36m_filter_classes\u001b[1;34m(df, y_col, classes)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ],
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the training the validation data can be loaded.\n",
    "You will need to supply the appropriate directory. Usually each pixels varies from 0-255, but it's highly recommended to normalize them in range of 0-1 to speed up the training process.\n",
    "The dataloader also do a simple normalisation on the pixel values directly.\n",
    "\n",
    "Now we can use a simple model.fit\\_generator() in Keras to train the model. However you will not be doing this. In order to understand the process we will write a function that iterates through the examples and train the model. First we need a function to calculate the accuracy and the loss for a given image stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_losses(model_in, data_generator_in, N_images, batch_size_):\n",
    "    loss_hold = []\n",
    "    acc_hold = []\n",
    "    batches = 0\n",
    "    \n",
    "    # iterate over each batch\n",
    "    for x,y in data_generator_in:\n",
    "        loss, acc = model_in.evaluate(x, y, verbose=0)\n",
    "        loss_hold.append(loss)\n",
    "        acc_hold.append(acc)\n",
    "        batches += 1\n",
    "        if batches >= N_images / batch_size_:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "            \n",
    "    return np.mean(loss_hold), np.mean(acc_hold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand the above function yourself. \n",
    "\n",
    "Next lets write a function to train a model train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_, num_epoch=50, verbose=False):\n",
    "    res = []\n",
    "    for e in range(num_epoch):\n",
    "        # print('Epoch', e)\n",
    "        batches = 0\n",
    "\n",
    "        loss_ = []\n",
    "        acc_ = []\n",
    "\n",
    "         # iterate over each batch\n",
    "        for x,y in train_generator:\n",
    "            loss, acc = model_.train_on_batch(x, y) # Update weights and return train loss, acc per batch\n",
    "            loss_.append(loss)\n",
    "            acc_.append(acc)\n",
    "            batches += 1\n",
    "            if batches >= N_train_images / batch_size:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "        loss_ = np.mean(loss_)\n",
    "        acc_ = np.mean(acc_)\n",
    "\n",
    "        loss, acc = calculate_losses(model_, validation_generator, N_val_images, batch_size)\n",
    "        if verbose:\n",
    "            print(\"Training epoch {}: Loss = {}, Accuracy = {}\".format(e, loss_, acc_))\n",
    "            print(\"Validation epoch {}: Loss = {}, Accuracy = {}\".format(e, loss, acc))\n",
    "\n",
    "        res.append((e, loss_, acc_, loss, acc))\n",
    "    return np.asarray(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "res = train_model(model_orig, num_epoch=250, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a function to plot the results and plot the error loss for the training and accuracy data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_results(res):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(res[:,0], res[:,1], 'r-')\n",
    "    plt.plot(res[:,0], res[:,3], 'b-')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([0, np.max([5., np.max(res[:,1]), np.max(res[:,3])])])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(res[:,0], res[:,2], 'r-')\n",
    "    plt.plot(res[:,0], res[:,4], 'b-')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, np.max([1., np.max(res[:,2]), np.max(res[:,4])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_results(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the above model overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some regularization\n",
    "\n",
    "Lets add some ridge penalty and create a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "\n",
    "\n",
    "# Input layer\n",
    "input_ = Input(shape=(28, 28, 3)) # This is the input shape\n",
    "input_slice = Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1, name=None))(input_)\n",
    "x = Flatten()(input_slice)  # This will convert the 28x28 input to a vector of  dimension 784\n",
    "\n",
    "# Hidden layer\n",
    "h = Dense(64,kernel_regularizer=regularizers.l2(0.01))(x) # reg for weight matrix 1\n",
    "h = Activation('sigmoid', name='hidden_layer')(h)\n",
    "\n",
    "# Output layer\n",
    "out_ = Dense(4,kernel_regularizer=regularizers.l2(0.01))(h) # reg for weight matrix 2\n",
    "out_ = Activation('softmax')(out_)\n",
    "\n",
    "# Define model\n",
    "model_reg = Model(inputs=input_, outputs=out_)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_reg.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=[categorical_accuracy])\n",
    "\n",
    "\n",
    "model_reg.summary()\n",
    "\n",
    "# Creating a model for feature vizualization (will be explained later)\n",
    "hidden_features = Model(inputs=input_, outputs=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "res = train_model(model_reg, num_epoch=250, verbose=False)\n",
    "plot_results(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the model still overfitting? How can you tune this parameter to balance overfitting/underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some dropout\n",
    "\n",
    "Lets create a model with dropput and see if that can also help with overfitting (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "\n",
    "# Input layer\n",
    "input_ = Input(shape=(28, 28, 3)) # This is the input shape\n",
    "input_slice = Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1, name=None))(input_)\n",
    "x = Flatten()(input_slice)  # This will convert the 28x28 input to a vector of  dimension 784\n",
    "\n",
    "# Hidden layer\n",
    "h = Dense(64)(x) \n",
    "h = Activation('sigmoid')(h)\n",
    "h= Dropout(rate=0.5)(h)\n",
    "\n",
    "# Output layer\n",
    "out_ = Dense(4)(h) \n",
    "out_ = Activation('softmax')(out_)\n",
    "\n",
    "# Define model\n",
    "model_drop = Model(inputs=input_, outputs=out_)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_drop.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=[categorical_accuracy])\n",
    "\n",
    "\n",
    "model_drop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "res = train_model(model_drop, num_epoch=250, verbose=False)\n",
    "plot_results(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the dropout and check if you can reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on random images from test set\n",
    "\n",
    "Since we dont have an independet test set for this data, we will test on the validation set. This is clearly not a good stratergy. \n",
    "\n",
    "First we will create a data generator to get the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size_t = 1\n",
    "\n",
    "# Here the validation is used for testing and this should be changes to a separate test set.\n",
    "test_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size_t,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "d_inv = {v: k for k, v in d.items()}\n",
    "plt.figure(figsize=(16,4))\n",
    "batches = 0\n",
    "for x,y in test_generator:\n",
    "        batches = batches + 1\n",
    "        y_hat = model_reg.predict(x, verbose=0)\n",
    "        x = np.squeeze(x)\n",
    "        if batches < 5:\n",
    "            plt.subplot(1,5,batches)\n",
    "            plt.imshow(x)\n",
    "            plt.title(\"GT-{}, Pred-{}\".format(d_inv[np.argmax(y[0])], d_inv[np.argmax(y_hat[0])] ))\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the hidden layer features and vizualize\n",
    "\n",
    "For extracting features we can generate a new model with the same input as the original model and with the output from the desired layer. I have done this for the regularized model. Check the code block of the regularized model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#print layer names\n",
    "for layer in hidden_features.layers:\n",
    "    print(layer.name)\n",
    "\n",
    "\n",
    "def extract_hidden_features(model_, generator_, N_images): \n",
    "    batches = 0\n",
    "\n",
    "    features = []\n",
    "    inputs = []\n",
    "    y_ = []\n",
    "\n",
    "    # iterate over each batch\n",
    "    for x,y in generator_:\n",
    "        hf = model_.predict(x) \n",
    "        for h in hf:\n",
    "            features.append(h)\n",
    "        for yy in y:\n",
    "            y_.append(np.argmax(yy))\n",
    "        for xx in x:\n",
    "            inputs.append(xx.reshape(-1,))\n",
    "\n",
    "        batches += 1\n",
    "        if batches >= N_images / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break \n",
    "    features = np.asarray(features)\n",
    "    y = np.asarray(y_)\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    return inputs, y, features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "inputs, y, features = extract_hidden_features(hidden_features, train_generator, N_train_images)\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,2,2)\n",
    "features_embedded = TSNE(n_components=2).fit_transform(features)\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y)\n",
    "plt.title('Hidden Layer Embedding')\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "inputs_embedded = TSNE(n_components=2).fit_transform(inputs)\n",
    "plt.scatter(inputs_embedded[:,0], inputs_embedded[:,1], c=y)\n",
    "plt.title('Input Embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "inputs, y, features = extract_hidden_features(hidden_features, validation_generator, N_val_images)\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,2,2)\n",
    "features_embedded = TSNE(n_components=2).fit_transform(features)\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y)\n",
    "plt.title('Hidden Layer Embedding')\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "inputs_embedded = TSNE(n_components=2).fit_transform(inputs)\n",
    "plt.scatter(inputs_embedded[:,0], inputs_embedded[:,1], c=y)\n",
    "plt.title('Input Embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the above two plots tell you about overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a deep CNN network\n",
    "\n",
    "One important method that can be used to prevent CNN from overfitting to the training data, one can augment the images randomly. The image data generator can apply random image augmentations to data like: rotations, translation etc. Lets do some random rotations and translations. Note that Augmentations are not usually applied to test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last', \n",
    "                                   rotation_range=15, width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv2D(32, (3, 3),kernel_regularizer=regularizers.l2(0.001), input_shape=(28, 28, 3)))\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_cnn.add(Conv2D(32, (3, 3),kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_cnn.add(Activation('relu'))\n",
    "# model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_cnn.add(Conv2D(64, (3, 3)))\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_cnn.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model_cnn.add(Dense(64))\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(4))\n",
    "model_cnn.add(Activation('softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=[categorical_accuracy])\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "res = train_model(model_cnn, num_epoch=250, verbose=False)\n",
    "plot_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import boto3 \n",
    "# import botocore \n",
    "# import pandas as pd \n",
    "# from sagemaker import get_execution_role \n",
    "\n",
    "# role = get_execution_role() \n",
    "# 's3://ml-assignment2-data/UCI-electricity/UCI_data.csv'\n",
    "# bucket = 'ml-assignment2-data' \n",
    "# data_key = 'UCI-electricity/UCI_data.csv'\n",
    "# data_location = 's3://{}/{}'.format(bucket, data_key) \n",
    "\n",
    "# pd.read_csv(data_location) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}