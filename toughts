Use "same" to keep image the same size

0.01 learning rate causes validation data to jump


17th Jul, 2014
Julio David Buldain PÃ©rez
University of Zaragoza
If you have enough data, you can try Early Stopping method: divide data in three data sets,
 training, validation and evaluation.
 Train each network along a sufficient number of epochs to see the training Mean Squared Error to be stuck in a minimum.
  The training process uses training data-set and must be executed epoch by epoch,
   in order to calculate the Mean Squared Error of the network in each epoch for the validation set.
    The network for the epoch with the minimum validation MSE is selected for the evaluation process.
This approach can be applied also with regularization methods and k-fold cross


General:
Keep padding off of maxpool onto conv?
No square traffic signs in german dataset
Resizing images
Image size being too small

-----MODEL 1 SHAPES-----
learning rate of 0.1 too big
Batch Normalization decreased performance if used too much
I don't want to put in too many features in conv2d, the image is small and only really needs edge detection
Image Augmentation is having a large effect on the accuracy
Same because the image is 28x28 which is small
More featues doesn't improve performance much, whilst using increased computational power
0.01 regulaizer too large
Talk about padding on conv vs maxpool




-----MODEL 2 TYPES------------ Aiming for 95 of vali - try 72BS
kernel reg doesn't have much effect on eariler or later layers, but does when it isn't used at all
dropout of 50% too much
learning rate good
0.0001 l2 reg doesn't do much
x2 64 CONV maybe overfitting
don't want to reduce the size too much, valuable information in those pixels


TODO try more features